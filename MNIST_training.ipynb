{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/reset_splines.pkl', 'rb') as f:\n",
    "    reset_splines = pickle.load(f)\n",
    "with open('data/set_splines.pkl', 'rb') as f:\n",
    "    set_splines = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 14*14  # 28*28 downsampled images\n",
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset with downsampling\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((14, 14)),     # Downsample to 7x7\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = MNIST(root='./data/MNIST', train=True, transform=transform, download=True)\n",
    "test_dataset = MNIST(root='./data/MNIST', train=False, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardwareAwareOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.01, set_splines=None, reset_splines = None):\n",
    "        defaults = dict(lr=lr)\n",
    "        self.set_splines = set_splines\n",
    "        self.reset_splines = reset_splines\n",
    "        super(HardwareAwareOptimizer, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        def map_weights_to_conductance(weight, min_weight, max_weight, min_conductance, max_conductance):\n",
    "            # Linear interpolation from weight to conductance\n",
    "            normalized_weights = (weight - min_weight) / (max_weight - min_weight)\n",
    "            mapped_conductance = min_conductance + normalized_weights * (max_conductance - min_conductance)\n",
    "            mapped_conductance = torch.clamp(mapped_conductance, min_conductance, max_conductance)\n",
    "            return mapped_conductance\n",
    "        \n",
    "        def new_conductance(G0, grad):\n",
    "            \n",
    "            # G_pos = G0*1.05\n",
    "            # G_neg = G0*(0.95)\n",
    "            # Apply the splines based on gradient\n",
    "            positive_mask = torch.le(grad, 0)  # Check for grad >= 0\n",
    "            G_pos = G0 * (self.set_splines['1 us'].ev(G0, 2.5))\n",
    "            G_neg = G0 * (self.reset_splines['1 us'].ev(G0, -2.5))\n",
    "            \n",
    "            # # Combine both positive and negative cases\n",
    "            G_new = torch.where(positive_mask, G_pos, G_neg)\n",
    "            return G_new\n",
    "\n",
    "        def map_conductance_to_weights(conductance, min_conductance, max_conductance, min_weight, max_weight):\n",
    "            # Reverse mapping from conductance to weight\n",
    "\n",
    "            normalized_conductance = (conductance - min_conductance) / (max_conductance - min_conductance)\n",
    "            mapped_weight = min_weight + normalized_conductance * (max_weight - min_weight)\n",
    "            mapped_weight = torch.clamp(mapped_weight, min_weight, max_weight)\n",
    "            return mapped_weight\n",
    "\n",
    "        def map_weights_to_conductance_sine(weight, min_weight, max_weight, min_conductance, max_conductance):\n",
    "            mid_weight = (max_weight+min_weight)/2\n",
    "            mid_conductance = (max_conductance + min_conductance)/2\n",
    "            normalized_weight = (weight - mid_weight)/(max_weight-mid_weight)\n",
    "            normalized_conductance = 2/torch.pi * torch.arcsin(normalized_weight)\n",
    "            conductance = normalized_conductance * (max_conductance - mid_conductance) + mid_conductance\n",
    "            return conductance\n",
    "        \n",
    "        def map_conductance_to_weights_sine(conductance, min_weight, max_weight, min_conductance, max_conductance):\n",
    "            mid_weight = (max_weight+min_weight)/2\n",
    "            mid_conductance = (max_conductance + min_conductance)/2\n",
    "            normalized_conductance = (conductance - mid_conductance)/(max_conductance - mid_conductance)\n",
    "            normalized_weight = torch.sin(normalized_conductance * torch.pi/2)\n",
    "            weight = normalized_weight*(max_weight - mid_weight) + mid_weight\n",
    "            return weight\n",
    "\n",
    "\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                min_weight, max_weight = -1.5, 1.5\n",
    "                min_conductance, max_conductance = 5e-7, 6e-6\n",
    "                \n",
    "                # Get the current weights and gradients\n",
    "                weight, grad = p.data, p.grad.data\n",
    "\n",
    "                # 1. Map weights to conductance values\n",
    "                G0 = map_weights_to_conductance_sine(\n",
    "                    weight=weight,\n",
    "                    min_weight=min_weight,\n",
    "                    max_weight=max_weight,\n",
    "                    min_conductance=min_conductance,\n",
    "                    max_conductance=max_conductance\n",
    "                )\n",
    "\n",
    "                # 2. Modify conductance using spline function based on the gradient\n",
    "                G_new = new_conductance(G0=G0, grad=grad)\n",
    "\n",
    "                # 3. Map the new conductance values back to weights\n",
    "                new_weight = map_conductance_to_weights_sine(\n",
    "                    conductance=G_new,\n",
    "                    min_conductance=min_conductance,\n",
    "                    max_conductance=max_conductance,\n",
    "                    min_weight=min_weight,\n",
    "                    max_weight=max_weight\n",
    "                )\n",
    "                \n",
    "                # Update the weights in the optimizer\n",
    "                p.data.copy_(new_weight)  # In-place update of the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP model with no hidden layers\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, input_size)  # Flatten the input\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1970\n",
      "Epoch [1/10], Loss: 1.6776\n",
      "Epoch [2/10], Loss: 1.7287\n",
      "Epoch [3/10], Loss: 1.7394\n",
      "Epoch [4/10], Loss: 1.8299\n",
      "Epoch [5/10], Loss: 1.7140\n",
      "Epoch [6/10], Loss: 1.7114\n",
      "Epoch [7/10], Loss: 1.6217\n",
      "Epoch [8/10], Loss: 1.5759\n",
      "Epoch [9/10], Loss: 1.6088\n",
      "Epoch [10/10], Loss: 1.6846\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "model = SimpleMLP(input_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = HardwareAwareOptimizer(model.parameters(), lr=0.01, set_splines=set_splines, reset_splines=reset_splines)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Check total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10,000 test images: 82.97%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, targets in test_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy of the model on the 10,000 test images: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
